{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'normalize_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d2ea8a1000dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGPy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from diGP.preprocessing import (readHCP,\n\u001b[0m\u001b[1;32m      6\u001b[0m                                 \u001b[0maverageb0Volumes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                 \u001b[0mcreateBrainMaskFromb0Data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'normalize_data'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import GPy\n",
    "from diGP.preprocessing import (readHCP,\n",
    "                                averageb0Volumes,\n",
    "                                createBrainMaskFromb0Data,\n",
    "                                replaceNegativeData,\n",
    "                                normalize_data)\n",
    "from diGP.dataManipulations import (DataHandler,\n",
    "                                    log_q_squared,\n",
    "                                    generateCoordinates)\n",
    "from diGP.generateSyntheticData import combineCoordinatesAndqVecs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = 'C:\\\\Users\\\\sesjojen\\\\Documents\\\\Data\\\\HumanConnectomeProject\\\\mgh_1007\\\\diff\\\\preproc'\n",
    "print(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gtab, data, voxelSize = readHCP(dataPath)\n",
    "print(gtab.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = replaceNegativeData(data, gtab)\n",
    "\n",
    "b0 = averageb0Volumes(data, gtab)\n",
    "mask = createBrainMaskFromb0Data(b0)\n",
    "data = normalize_d\n",
    "#maskIdx = np.nonzero(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def normalize_signal(data, maskIdx, b0):\n",
    "    S = data[maskIdx[0], maskIdx[1], maskIdx[2], :]\n",
    "    S = S/b0[maskIdx[0], maskIdx[1], maskIdx[2], np.newaxis]\n",
    "    maxSignal = 1.5\n",
    "    percentExceedingMaxSignal = 100*np.sum(S>maxSignal)/np.prod(S.shape)\n",
    "    print('Replacing the top {} % values with {}.'.format(percentExceedingMaxSignal, maxSignal))\n",
    "    S[S > maxSignal] = maxSignal\n",
    "    return S\n",
    "\n",
    "S = normalize_signal(data, maskIdx, b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to detect problems further down the road, we will for now reduce the spatial extent used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#handler = DataHandler(gtab, data[60:70, 60:70, 25:55:2, :]/b0[60:70, 60:70, 25:55:2, None] + 1e-6,\n",
    "#                      voxelSize=(voxelSize[0], voxelSize[1], 2*voxelSize[2]), qMagnitudeTransform=log_q_squared)\n",
    "handler = DataHandler(gtab, data[:, :, ::2, :], voxelSize=(voxelSize[0], voxelSize[1], 2*voxelSize[2]), qMagnitudeTransform=log_q_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spatialLengthScale = 5\n",
    "bValLengthScale = 3\n",
    "\n",
    "kernel = (GPy.kern.RBF(input_dim=1, active_dims=[0],\n",
    "                       variance=1,\n",
    "                       lengthscale=spatialLengthScale) *\n",
    "          GPy.kern.RBF(input_dim=1, active_dims=[1],\n",
    "                       variance=1,\n",
    "                       lengthscale=spatialLengthScale) *\n",
    "          GPy.kern.RBF(input_dim=1, active_dims=[2],\n",
    "                       variance=1,\n",
    "                       lengthscale=spatialLengthScale) *\n",
    "          GPy.kern.Matern52(input_dim=1, active_dims=[3],\n",
    "                            variance=1,\n",
    "                            lengthscale=bValLengthScale) *\n",
    "          GPy.kern.LegendrePolynomial(\n",
    "             input_dim=3,\n",
    "             coefficients=np.array((2, 0.5, 0.05)),\n",
    "             orders=(0, 2, 4),\n",
    "             active_dims=(4, 5, 6)))\n",
    "\n",
    "kernel.parts[0].variance.fix(value=1)\n",
    "kernel.parts[1].variance.fix(value=1)\n",
    "kernel.parts[2].variance.fix(value=1)\n",
    "kernel.parts[3].variance.fix(value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "handler.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_dims = [[0], [1], [2], [3, 4, 5, 6]]\n",
    "\n",
    "model = GPy.models.GPRegressionGrid(handler.X, handler.y, kernel, grid_dims=grid_dims)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimize(messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(\"Legendre coefficients: {}\".format(model.mul.LegendrePolynomial.coefficients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test, let's try doing some simple spatial interpolation. Specifically, let X be every second slice and Xnew those in between. More advanced would be a checker board pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "handlerPred = DataHandler(gtab, data[:, :, 1::2, :],\n",
    "                          voxelSize=(voxelSize[0], voxelSize[1], 2*voxelSize[2]), qMagnitudeTransform=log_q_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "inspect.getfile(model.predict_noiseless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = model.predict_noiseless(handlerPred.X, compute_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results: slice below, interpolated, slice above\n",
    "Also show predictive variance? Have to think about current implementation of prediction, which will probably be too memory intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(handlerPred.y-mu, bins=500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yTrue = np.reshape(handler.y, handler.originalShape)\n",
    "mu = mu.reshape(handlerPred.originalShape)\n",
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "f, axs = plt.subplots(5, 3)\n",
    "f.set_figheight(16*3)\n",
    "f.set_figwidth(16)\n",
    "for i in range(5):\n",
    "    axs[i, 0].imshow(yTrue[:, :, 5, i], vmin=0, vmax=1)\n",
    "    axs[i, 1].imshow(mu[:, :, 5, i], vmin=0, vmax=1)\n",
    "    axs[i, 2].imshow(yTrue[:, :, 6, i], vmin=0, vmax=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "8ab0fcb39bd64672abde09b72be80911": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
